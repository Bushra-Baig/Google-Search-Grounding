{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuC_ceHQwT2b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from IPython.display import Markdown, display\n",
        "from google.api_core import retry\n",
        "\n",
        "# Get API Key\n",
        "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
        "if not GOOGLE_API_KEY:\n",
        "    GOOGLE_API_KEY = input(\"Please enter your Google API key: \")\n",
        "if not GOOGLE_API_KEY:\n",
        "    raise ValueError(\"Google API key not found. Please set the GOOGLE_API_KEY environment variable or provide it manually.\")\n",
        "\n",
        "# Initialize GenAI client\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retry policy for quota or service limit errors\n",
        "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
        "\n",
        "# Apply retry only once to avoid multiple wrappings\n",
        "if not hasattr(genai.models.Models.generate_content, '_wrapped_'):\n",
        "    genai.models.Models.generate_content = retry.Retry(predicate=is_retriable)(\n",
        "        genai.models.Models.generate_content\n",
        "    )\n"
      ],
      "metadata": {
        "id": "S8DoMvZZw0MH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable grounding via Google Search\n",
        "config_with_search = types.GenerateContentConfig(\n",
        "    tools=[types.Tool(google_search=types.GoogleSearch())],\n",
        ")\n"
      ],
      "metadata": {
        "id": "F6z8i0Yvw5ns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chat memory (list of alternating user and model messages)\n",
        "conversation_history = []\n"
      ],
      "metadata": {
        "id": "HGcaIvsHzO2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Chatbot with memory and search grounding started! Type 'quit' to exit.\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() == 'quit':\n",
        "        print(\"Exiting the chatbot. Goodbye!\")\n",
        "        break\n",
        "\n",
        "    # Add user message to memory with correct Part format\n",
        "    conversation_history.append(types.Content(role=\"user\", parts=[types.Part(text=user_input)]))\n",
        "\n",
        "    try:\n",
        "        # Send full conversation context to model\n",
        "        response = client.models.generate_content(\n",
        "            model=\"gemini-2.0-flash\",\n",
        "            contents=conversation_history,\n",
        "            config=config_with_search,\n",
        "        )\n",
        "\n",
        "        # Get and display response\n",
        "        reply_text = response.candidates[0].content.parts[0].text\n",
        "        display(Markdown(reply_text))\n",
        "\n",
        "        # Add model reply to memory correctly\n",
        "        conversation_history.append(types.Content(role=\"model\", parts=[types.Part(text=reply_text)]))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred:\", e)\n"
      ],
      "metadata": {
        "id": "rkdmEhC-0DRk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}